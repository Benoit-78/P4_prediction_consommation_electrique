{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# I. Introduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problématique**\n",
    "\n",
    "\n",
    "La ville de Seattle aimerait réduire ses émissions de C02 et diminuer sa consommation en énergie, en particulier au niveau de ses bâtiments.\n",
    "\n",
    "Via des mesures de chaleurs, on peut estimer le dégagement carbonné des bâtiments de la ville. Problème : ces mesures de chaleurs sont coûteuses, et il est inenvisageable de les déployer sur l'ensemble des bâtiments de la ville.\n",
    "\n",
    "**Buts et intérêts**\n",
    "\n",
    "\n",
    "L'objectif du projet est de prédire les émissions et consommations des bâtiments de la ville. Le programme va tout d'abord fournir quelques analyses du jeu de données à disposition, normaliser les données utiles, puis finalement utiliser quelques algorithmes de Machine Learning pour établir des prédictions.\n",
    "\n",
    "L'intérêt d'un tel programme est de nous affranchir de réaliser les mesures de chaleur onéreuses. Le programme fournit également le niveau de fiabilité qu'on peut accorder aux différentes prédictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# II. Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import statistics as stat\n",
    "import sklearn.model_selection\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from matplotlib.pyplot import figure\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import std_eda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sklearn.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# III. Data presentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Présentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CHEMIN = 'drive/My Drive/Colab Notebooks/ocr_data_scientist/P4 Consommation électrique des bâtiments/'\n",
    "FILENAME_2015 = '2015-building-energy-benchmarking.csv'\n",
    "FILENAME_2016 = '2016-building-energy-benchmarking.csv'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main2015_df = pd.read_csv(CHEMIN + FILENAME_2015, sep=',')\n",
    "main2016_df = pd.read_csv(CHEMIN + FILENAME_2016, sep=',')\n",
    "my_explorator = std_eda.EdaExplorator(main2015_df, main2016_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main2015_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main2016_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def NaN_proportion(df):\n",
    "    '''Returns the proportion of NaN values in the whole dataframe'''\n",
    "    nan_proportion = df.isna().sum().sum() / (df.shape[0] * df.shape[1]) * 100\n",
    "    return nan_proportion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(main2015_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Fusion des tableaux 2015 & 2016"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identification des colonnes différentes d'une année à l'autre\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns_2015 = [column for column in main2015_df.columns]\n",
    "columns_2016 = [column for column in main2016_df.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Columns in common: \\n')\n",
    "commons_list = list(set(columns_2015) & set(columns_2016))\n",
    "commons_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "specific_2016 = list(set(columns_2016) - set(columns_2015))\n",
    "specific_2016"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "specific_2015 = list(set(columns_2015) - set(columns_2016))\n",
    "specific_2015"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici, nous repérons deux couples de colonnes ayant un nom similaire d'une année à l'autre :\n",
    "\n",
    "\n",
    "*   GHGEmissions(MetricTonsCO2e) & TotalGHGEmissions\n",
    "*   GHGEmissionsIntensity(kgCO2e/ft2) & GHGEmissionsIntensity\n",
    "\n",
    "Nous allons voir si ces colonnes aux noms similaires désignent la même caractéristique.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main2015_df['GHGEmissionsIntensity(kgCO2e/ft2)'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "main2016_df['GHGEmissionsIntensity'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous trouvons des colonnes de noms différents, mais similaires. Donnons-leur le même nom dans les deux dataframes, 2015 et 2016."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Nettoyage des données du tableau 2015\n",
    "main2015_df = main2015_df.rename(columns={'Comment':'Comments',\n",
    "                                          'GHGEmissions(MetricTonsCO2e)':'GHGEmissionsTotal',\n",
    "                                          'GHGEmissionsIntensity(kgCO2e/ft2)':'GHGEmissionsIntensity',\n",
    "                                          'Zip Codes':'ZipCode'})\n",
    "\n",
    "# Nettoyage des données du tableau 2016\n",
    "main2016_df = main2016_df.drop(['Address', 'City', 'State'], axis=1)\n",
    "main2016_df = main2016_df.rename(columns={'TotalGHGEmissions':'GHGEmissionsTotal'})\n",
    "\n",
    "columns_2015 = [column for column in main2015_df.columns]\n",
    "columns_2016 = [column for column in main2016_df.columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Columns in common: \\n')\n",
    "commons_list = list(set(columns_2015) & set(columns_2016))\n",
    "commons_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Columns contained in 2016 dataframe but not in 2015')\n",
    "specific_2016 = list(set(columns_2016) - set(columns_2015))\n",
    "specific_2016"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Columns contained in 2015 dataframe but not in 2016')\n",
    "specific_2015 = list(set(columns_2015) - set(columns_2016))\n",
    "specific_2015"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Détermination des clés de chaque tableau"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def columns_with_unique_values(df):\n",
    "    df_length = len(df)\n",
    "    temp_dict={}\n",
    "    for column in df.columns:\n",
    "        temp_dict[column] = len(df[column].unique())\n",
    "    temp_list = [element[0] for element in temp_dict.items() if element[1]==temp_value]\n",
    "    return temp_list\n",
    "\n",
    "columns_with_unique_values(main2015_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns_with_unique_values(main2016_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La colonne OSEBuilding est la clé pour 2015 et pour 2016."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Estimation des différences colonne par colonne entre les deux années**\n",
    "\n",
    "Pour cela, nous allons comparer les éléments de 2015 et 2016 clé par clé."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fusion des deux tableaux, en prenant les clés pour indice\n",
    "merge_df = main2015_df.merge(main2016_df,\n",
    "                             left_on='OSEBuildingID',\n",
    "                             right_on='OSEBuildingID',\n",
    "                             suffixes=('_2015', '_2016'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Vérification des résultats sur quelques colonnes :\n",
    "# YearBuilt : env. 100%\n",
    "# PropertyGFABuilding(s) : env. 85%\n",
    "# ZipCode : env. 0%\n",
    "merge_df['ZipCode_2015'][:10], merge_df['ZipCode_2016'][:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Fusion des deux tableaux**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Dimensions du tableau de 2015 : {}'.format(main2015_df.shape))\n",
    "print('Dimensions du tableau de 2016 : {}'.format(main2016_df.shape))\n",
    "print('Dimensions du tableau fusionné : {}'.format(merge_df.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On voit que le tableau fusionné contient moins de lignes que les deux tableaux de départ. Cela veut dire que certains bâtiments ont été passés en revue en 2015, mais pas en 2016, et vice-versa.\n",
    "\n",
    "Il serait dommage de laisser de côté ces bâtiments. Pour une modélisation de Machine Learning, il est préférable d'avoir le plus de données possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Récupérer la liste des bâtiments propres à 2015\n",
    "buildings2015only = set(main2015_df['OSEBuildingID']) - set(merge_df['OSEBuildingID'])\n",
    "buildings2015only = list(buildings2015only)\n",
    "\n",
    "# Récupérer la liste des bâtiments propres à 2016\n",
    "buildings2016only = set(main2016_df['OSEBuildingID']) - set(merge_df['OSEBuildingID'])\n",
    "buildings2016only = list(buildings2016only)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lister les bâtiments de 2015 et leurs caractéristiques\n",
    "temp2015_df = main2015_df[main2015_df['OSEBuildingID'].isin(buildings2015only)]\n",
    "# Eliminer la colonne OSEBuildingID\n",
    "temp2015_df.drop('OSEBuildingID', axis=1, inplace=True)\n",
    "for column_ in temp2015_df.columns:\n",
    "    if column_ in commons_list:\n",
    "        temp2015_df = temp2015_df.rename(columns={column_: column_ + '_2015'})\n",
    "\n",
    "# Lister les bâtiments de 2016 et leurs caractéristiques\n",
    "temp2016_df = main2016_df[main2016_df['OSEBuildingID'].isin(buildings2016only)]\n",
    "temp2016_df.drop('OSEBuildingID', axis=1, inplace=True)\n",
    "for column_ in temp2016_df.columns:\n",
    "    if column_ in commons_list:\n",
    "        temp2016_df = temp2016_df.rename(columns={column_: column_ + '_2016'})\n",
    "\n",
    "# Concaténer les tableaux au tableau fusionné\n",
    "merge_df = pd.concat([merge_df, temp2015_df, temp2016_df])\n",
    "\n",
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ranger les colonnes par ordre alphabétique\n",
    "merge_df = merge_df.reindex(sorted(merge_df.columns), axis=1)\n",
    "merge_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IV. Exploratory analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Introduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons explorer les données qualitatives, puis les données quantitatives.\n",
    "\n",
    "Nous voulons :\n",
    "- estimer la pertinence de chaque catégorie de donnée,\n",
    "- comprendre le contenu de chaque catégorie,\n",
    "- ne retenir que les catégories vraiment utiles à notre analyse, c'est-à-dire uniquement les grandeurs dont nous pensons qu'elles peuvent avoir une influence sur les valeurs cibles (les *targets*).\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyses séparées des variables quantitives et des variables qualitatives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Données qualitatives\n",
    "qualitative_columns = [column for column in merge_df.columns if merge_df[column].dtype == object]\n",
    "qualitative_df = merge_df[qualitative_columns]\n",
    "\n",
    "# Données quantitatives\n",
    "quantitative_columns = [column for column in merge_df.columns if merge_df[column].dtype != object]\n",
    "quantitative_df = merge_df[quantitative_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape, quantitative_df.shape, qualitative_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fonction de représentation des données non-NaN pour chaque colonne"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def non_NaN_histogram(df):\n",
    "    \"\"\"Returns the proportion of non-NaN values for each column of a dataframe.\n",
    "    Results are given under the form of an histogram.\"\"\"\n",
    "    columns_nan = {}\n",
    "    for column in df.columns:\n",
    "        columns_nan[column] = df[column].notna().sum()\n",
    "    columns_nan = sorted(columns_nan.items(), key=lambda item: item[1], reverse=False)\n",
    "    columns = [item[0] for item in columns_nan]\n",
    "    nan_proportions = [item[1]/df.shape[0] for item in columns_nan]\n",
    "    # Determine the color of the bars according to the type of the data\n",
    "    if df[column].dtype != object:\n",
    "        color = 'blue'\n",
    "        title = 'Proportion de données non-Nan quantitatives'\n",
    "    else:\n",
    "        color = 'orange'\n",
    "        title = 'Proportion de données non-Nan qualitatives'\n",
    "    plt.figure(figsize=(5, math.sqrt(3*len(columns))))\n",
    "    plt.title('{} ({})'.format(title, len(columns_nan)))\n",
    "    plt.barh(columns, nan_proportions, color=color, edgecolor='k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Qualitative columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. Cleaning : how clean are our data ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_NaN_histogram(qualitative_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La colonne Location, de 2015, contient la latitude et la longitude. Ces deux données sont contenues dans le tableau de 2016."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminons les colonnes Comments et Outlier, qui contiennent trop peu de données."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['Comments_2015', 'Outlier_2015', 'Outlier_2016'], axis=1, inplace=True)\n",
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. Exploration : what do our data contain ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Et en particulier, quelles sont les colonnes susceptibles de révéler les bâtiments destinés à l'habitation ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qualitative_df.head(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "qualitative_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyse colonne par colonne"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_pareto(merge_df, 'BuildingType_2015')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Colonne PropertyName : Pareto des mots les plus récurrents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def most_common_words(df, column, separator, first_words=10):\n",
    "    raw_series = df[column]\n",
    "    words = []\n",
    "    for element in raw_series:\n",
    "        if type(element) == float:\n",
    "            words.append(np.nan)\n",
    "        else:\n",
    "            for word in element.split(separator):\n",
    "                words.append(word)\n",
    "    short_list = Counter(words).most_common(first_words)\n",
    "    words = [item[0] for item in short_list]\n",
    "    df_length = df.shape[0]\n",
    "    count = [item[1]/df_length*100 for item in short_list]\n",
    "    return words, count\n",
    "\n",
    "most_common_words(merge_df, 'PropertyName_2015', ' ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Colonne ListOfAllPropertyUseTypes : nettoyage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "most_common_words(merge_df, 'ListOfAllPropertyUseTypes_2016', ',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ces colonnes contiennent-elles des données utiles pour notre étude ?\n",
    "Ce qui nous intéresse surtout, c'est l'utilisation principale du bâtiment, qui est déjà contenue dans la colonne LargestPropertyUseType. Nous pouvons donc supprimer ces deux colonnes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['ListOfAllPropertyUseTypes_2015',\n",
    "               'ListOfAllPropertyUseTypes_2016'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Colonne YearsENERGYSTARCertified : nettoyage\n",
    "\n",
    "Certaines cellules contiennent non pas une, mais plusieurs valeurs. Cela correspond à un bâtiment ayant été certifié à plusieurs reprises."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Attention, ça ne va pas du tout\n",
    "\n",
    "def years_energystar_certified(df, column, separator):\n",
    "    years_series = df[column]\n",
    "    complete_list = []\n",
    "    for element in years_series:\n",
    "        if type(element) == str:\n",
    "            try:\n",
    "                len(element.split(separator)) > 1\n",
    "                for year in element.split(separator):\n",
    "                    complete_list.append(year)\n",
    "            except AttributeError:\n",
    "                complete_list.append(element)\n",
    "    complete_list = [int(year) for year in complete_list]\n",
    "\n",
    "complete_list = years_energystar_certified(main2016_df,\n",
    "                                           'YearsENERGYSTARCertified',\n",
    "                                           ',')\n",
    "\n",
    "# Certaines \"années\" sont en réalité une liste d'années, ou une concaténation d'années. Séparons-les :\n",
    "for _ in range(5):\n",
    "    for element in complete_list:\n",
    "        if type(element) == int:\n",
    "            str_element = str(element)\n",
    "            if len(str_element) > 4:\n",
    "                new_elements = [int(str_element[i: i+4]) for i in range(0, len(str_element), 4)]\n",
    "                complete_list.remove(element)\n",
    "                for year in new_elements:\n",
    "                    complete_list.append(year)\n",
    "# Create the counter\n",
    "years_dict = {}\n",
    "for key, value in Counter(complete_list).items():\n",
    "    years_dict[key] = value\n",
    "# Plot the graph\n",
    "x = range(1999, 2019)\n",
    "plt.xticks(x, x, rotation = 45)\n",
    "plt.xlim((1999, 2019))\n",
    "#plt.ylim((0, 60))\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Number of certifications')\n",
    "plt.title('YearsENERGYSTARCertified by year')\n",
    "plt.bar(list(years_dict.keys()), list(years_dict.values()), color='orange', edgecolor='k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On voit que certains bâtiments ont déjà reçu en 2016 leur certification pour 2017. S'agit-il de valeurs aberrantes, ou bien le score peut-il être décerné par avance ? Dans le doute, nous allons garder ces données."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons remplacer cette simple colonne par autant de colonnes qu'il y a eu d'années de certification. Ce afin d'avoir des données exploitables pour la modélisation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Année 2015\n",
    "def add_certif_year_cumsum(df, column, years_dict, prefix='ENERGYSTAR_'):\n",
    "    min_year = min(list(years_dict.keys()))\n",
    "    max_year = max(list(years_dict.keys()))\n",
    "    years_range = range(min_year, max_year)\n",
    "    temp_series = df[column]\n",
    "    for year in years_range:\n",
    "        year_list = []\n",
    "        for element in temp_series:\n",
    "            # Prévention du cas NaN\n",
    "            try:\n",
    "                if str(year) in element:\n",
    "                    year_list.append(1)\n",
    "                else:\n",
    "                    year_list.append(0)\n",
    "            # Cas NaN\n",
    "            except TypeError:\n",
    "                year_list.append(0)\n",
    "        temp_df = df.copy()\n",
    "    temp_df[prefix + str(year)] = year_list\n",
    "    return temp_df\n",
    "\n",
    "merge_df = add_certif_year_cumsum(merge_df, 'YearsENERGYSTARCertified_2015', years_dict)\n",
    "\n",
    "# Année 2016\n",
    "# L'année 2016 est beaucoup moins remplie, nous ne la gardons pas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop('YearsENERGYSTARCertified_2015', axis=1, inplace=True)\n",
    "merge_df.drop('YearsENERGYSTARCertified_2016', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Colonne Location : aperçu visuel de la répartition\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Données de 2015\n",
    "def represent_location_data(df, column):\n",
    "    locations_series = df[column]\n",
    "    coordinates_list = []\n",
    "    for element in locations_series:\n",
    "        latitude = element.split(',')[0].split(':')[-1].replace(' ', '')\n",
    "        latitude = latitude.split(',')[0].split(':')[-1].replace('\\'', '')\n",
    "        latitude = float(latitude)\n",
    "        longitude = element.split(',')[1].split(':')[-1].replace(' ', '')\n",
    "        longitude = longitude.split(',')[0].split(':')[-1].replace('\\'', '')\n",
    "        longitude = float(longitude)\n",
    "        coordinates_list.append((latitude, longitude))\n",
    "    latitudes = [coordinate[0] for coordinate in coordinates_list]\n",
    "    longitudes = [coordinate[1] for coordinate in coordinates_list]\n",
    "    plt.title('Coordonnées des bâtiments')\n",
    "    plt.xlabel('Longitudes')\n",
    "    plt.ylabel('Latitudes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axis('equal')\n",
    "    plt.scatter(longitudes, latitudes, color='orange', edgecolors='k', )\n",
    "\n",
    "represent_location_data(main2015_df, 'Location')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Données de 2016\n",
    "\n",
    "locations_df = main2016_df[['Latitude', 'Longitude']]\n",
    "\n",
    "plt.title('Coordonnées des bâtiments')\n",
    "plt.xlabel('Longitudes')\n",
    "plt.ylabel('Latitudes')\n",
    "plt.xticks(rotation=45)\n",
    "plt.axis('equal')\n",
    "plt.scatter(locations_df['Longitude'], locations_df['Latitude'], color='orange', edgecolors='k', )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Par la suite nous utiliserons uniquement les données géographiques de 2016."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df = merge_df.drop('Location', axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c. Analysis : which of our data are useful?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Commençons par enlever les colonnes dont nous sommes sûrs qu'elles ne seront pas utiles."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['ComplianceStatus_2015',\n",
    "               'ComplianceStatus_2016',\n",
    "               'PropertyName_2015',\n",
    "               'PropertyName_2016',\n",
    "               'SecondLargestPropertyUseType_2015',\n",
    "               'SecondLargestPropertyUseType_2016',\n",
    "               'ThirdLargestPropertyUseType_2015',\n",
    "               'ThirdLargestPropertyUseType_2016',\n",
    "               'TaxParcelIdentificationNumber_2015',\n",
    "               'TaxParcelIdentificationNumber_2016'], axis=1, inplace=True)\n",
    "\n",
    "qualitative_df.drop(['ComplianceStatus_2015',\n",
    "               'ComplianceStatus_2016',\n",
    "               'PropertyName_2015',\n",
    "               'PropertyName_2016',\n",
    "               'SecondLargestPropertyUseType_2015',\n",
    "               'SecondLargestPropertyUseType_2016',\n",
    "               'ThirdLargestPropertyUseType_2015',\n",
    "               'ThirdLargestPropertyUseType_2016',\n",
    "               'TaxParcelIdentificationNumber_2015',\n",
    "               'TaxParcelIdentificationNumber_2016'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les colonnes DefaultData contiennent des données similaires, mais de formats différents : Yes/No pour 2015, False/True pour 2016."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['DefaultData_2015'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['DefaultData_2016'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['DefaultData_2015'].replace({'No': 0, 'Yes':1}, inplace=True)\n",
    "merge_df['DefaultData_2016'].replace({False: 0, True:1}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Objectif : n'étudier que les bâtiments non destinés à l'habitation. Nous allons identifier les colonnes pouvant donner cette information, puis filtrer par ligne.\n",
    "\n",
    "Listons les colonnes en questions :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "buildingtype_list = main2016_df['BuildingType'].unique()\n",
    "primarypropertytype_list = main2016_df['PrimaryPropertyType'].unique()\n",
    "largestpropertyusetype_list = list(main2016_df['LargestPropertyUseType'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "largestpropertyusetype_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pour chaque colonne, nous allons identifier les catégories **non liées** à l'habitation, et que nous allons retenir pour notre étude."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Building type\n",
    "buildingtype2015_list = ['NonResidential', 'Nonresidential COS', 'SPS-District K-12', 'Campus']\n",
    "buildingtype2016_list = ['NonResidential', 'Nonresidential COS', 'SPS-District K-12',\n",
    "                         'Campus', 'Nonresidential WA']\n",
    "\n",
    "# Primary property type\n",
    "primarypropertytype2015_list = ['Hotel', 'Other', 'Mixed Use Property', 'K-12 School',\n",
    "                                'College/University', 'Small- and Mid-Sized Office',\n",
    "                                'Self-Storage Facility\\n', 'Distribution Center',\n",
    "                                'Large Office', 'Retail Store', 'Medical Office',\n",
    "                                'Hospital', 'Non-Refrigerated Warehouse', 'Distribution Center\\n',\n",
    "                                'SPS-District K-12', 'Worship Facility', 'Supermarket/Grocery Store',\n",
    "                                'Laboratory', 'Self-Storage Facility', 'Refrigerated Warehouse',\n",
    "                                'Restaurant\\n', 'Restaurant']\n",
    "primarypropertytype2016_list = ['Hotel', 'Other', 'Mixed Use Property', 'K-12 School',\n",
    "                                'University', 'Small- and Mid-Sized Office', 'Self-Storage Facility',\n",
    "                                'Warehouse', 'Large Office', 'Medical Office',\n",
    "                                'Retail Store', 'Hospital', 'Distribution Center',\n",
    "                                'Worship Facility', 'Supermarket / Grocery Store',\n",
    "                                'Laboratory', 'Refrigerated Warehouse', 'Restaurant',\n",
    "                                'Office']\n",
    "\n",
    "largestpropertyusetype2015_list = ['Hotel', 'Police Station', 'Other - Entertainment/Public Assembly',\n",
    "                                   np.nan, 'Library', 'Fitness Center/Health Club/Gym', 'Social/Meeting Hall',\n",
    "                                   'Courthouse', 'Other', 'K-12 School', 'College/University', 'Automobile Dealership',\n",
    "                                   'Office', 'Self-Storage Facility', 'Retail Store', 'Senior Care Community',\n",
    "                                   'Medical Office', 'Hospital (General Medical & Surgical)', 'Museum',\n",
    "                                   'Repair Services (Vehicle, Shoe, Locksmith, etc)', 'Other/Specialty Hospital',\n",
    "                                   'Financial Office', 'Non-Refrigerated Warehouse', 'Distribution Center',\n",
    "                                   'Parking', 'Worship Facility', 'Laboratory', 'Supermarket/Grocery Store',\n",
    "                                   'Convention Center', 'Urgent Care/Clinic/Other Outpatient', 'Other - Services',\n",
    "                                   'Strip Mall', 'Wholesale Club/Supercenter', 'Refrigerated Warehouse',\n",
    "                                   'Other - Recreation', 'Lifestyle Center', 'Other - Public Services',\n",
    "                                   'Data Center', 'Other - Mall', 'Manufacturing/Industrial Plant', 'Restaurant',\n",
    "                                   'Other - Education', 'Fire Station', 'Performing Arts', 'Bank Branch',\n",
    "                                   'Other - Restaurant/Bar', 'Food Service', 'Adult Education', 'Other - Utility',\n",
    "                                   'Movie Theater', 'Outpatient Rehabilitation/Physical Therapy',\n",
    "                                   'Personal Services (Health/Beauty, Dry Cleaning, etc)', 'Pre-school/Daycare']\n",
    "largestpropertyusetype2016_list = ['Hotel', 'Police Station', 'Other - Entertainment/Public Assembly',\n",
    "                                   'Library', 'Fitness Center/Health Club/Gym',\n",
    "                                   'Social/Meeting Hall', 'Courthouse', 'Other',\n",
    "                                   'K-12 School', 'College/University', 'Automobile Dealership',\n",
    "                                   'Office', 'Self-Storage Facility', 'Non-Refrigerated Warehouse',\n",
    "                                   'Other - Mall', 'Medical Office', 'Retail Store',\n",
    "                                   'Hospital (General Medical & Surgical)', 'Museum',\n",
    "                                   'Repair Services (Vehicle, Shoe, Locksmith, etc)',\n",
    "                                   'Other/Specialty Hospital', 'Financial Office',\n",
    "                                   'Distribution Center', 'Parking', 'Worship Facility',\n",
    "                                   'Restaurant', 'Data Center', 'Laboratory',\n",
    "                                   'Supermarket/Grocery Store', 'Convention Center',\n",
    "                                   'Urgent Care/Clinic/Other Outpatient', np.nan,\n",
    "                                   'Other - Services', 'Strip Mall', 'Wholesale Club/Supercenter',\n",
    "                                   'Refrigerated Warehouse', 'Manufacturing/Industrial Plant',\n",
    "                                   'Other - Recreation', 'Lifestyle Center', 'Other - Public Services',\n",
    "                                   'Other - Education', 'Fire Station', 'Performing Arts',\n",
    "                                   'Bank Branch', 'Other - Restaurant/Bar', 'Food Service',\n",
    "                                   'Adult Education', 'Other - Utility', 'Movie Theater',\n",
    "                                   'Personal Services (Health/Beauty, Dry Cleaning, etc)',\n",
    "                                   'Pre-school/Daycare', 'Prison/Incarceration']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filtrage des données concernant les bâtiments non destinés à l'habitation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Building type\n",
    "merge_df = merge_df[merge_df['BuildingType_2015'].isin(buildingtype2015_list)]\n",
    "merge_df = merge_df[merge_df['BuildingType_2016'].isin(buildingtype2016_list)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Primary property type\n",
    "merge_df = merge_df[merge_df['PrimaryPropertyType_2015'].isin(primarypropertytype2015_list)]\n",
    "merge_df = merge_df[merge_df['PrimaryPropertyType_2016'].isin(primarypropertytype2016_list)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Property use type\n",
    "merge_df = merge_df[merge_df['LargestPropertyUseType_2015'].isin(largestpropertyusetype2015_list)]\n",
    "merge_df = merge_df[merge_df['LargestPropertyUseType_2016'].isin(largestpropertyusetype2016_list)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le nombre de suppressions effectuées par les deux derniers nettoyages confirme le premier : plus de la moitié des bâtiments enregistrés étaient des bâtiments d'habitation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Quantitative columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. Cleaning : how clean are our data ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "non_NaN_histogram(quantitative_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantitative_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quelques données sont négatives, là où elles devraient être positives. Elles sont aberrantes et doivent être rejetées.\n",
    "\n",
    "Colonnes concernées : PropertyGFAParking, PropertyGFABuilding(s), SourceEUI(kBtu/sf), SourceEUIWN(kBtu/sf)\n",
    "\n",
    "\n",
    "Cela dit, il ne suffit pas de simplement rejeter ces données aberrantes en dessous de zéro. Il y a sûrement d'autres données erronées, certes au-dessus de zéro, mais sujettes à la même probabilité d'erreur. Il faudrait enquêter sur les raisons qui font que ces données sont apparues, et éradiquer la possibilité qu'à l'avenir, de telles données erronées ré-apparaissent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "erratic_list = ['Electricity(kBtu)_2016', 'Electricity(kWh)_2016', 'GHGEmissionsIntensity_2016',\n",
    "                'GHGEmissionsTotal_2016', 'PropertyGFABuilding(s)_2015', 'PropertyGFAParking_2015',\n",
    "                'PropertyGFAParking_2016', 'SourceEUIWN(kBtu/sf)_2015', 'SourceEUIWN(kBtu/sf)_2015',\n",
    "                'SourceEUI(kBtu/sf)_2015']\n",
    "\n",
    "for column_name in erratic_list:\n",
    "    quantitative_df = quantitative_df[quantitative_df[column_name] >= 0]\n",
    "    merge_df = merge_df[merge_df[column_name] >= 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Certains éléments sont des NaN. Ils ne sont pas vraiment représentatifs (moins de 0,2 %), nous pouvons les éliminer.\n",
    "merge_df = merge_df[merge_df['NumberofFloors_2015'].notnull()]\n",
    "merge_df = merge_df[merge_df['NumberofFloors_2016'].notnull()]\n",
    "\n",
    "quantitative_df = quantitative_df[quantitative_df['NumberofFloors_2015'].notnull()]\n",
    "quantitative_df = quantitative_df[quantitative_df['NumberofFloors_2016'].notnull()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Traitement du cas particulier de DefaultData**\n",
    "\n",
    "La colonne DefaultData contient des valeurs booléennes (True ou False). Ceci va nous contrarier pour le Feature Engineering.\n",
    "\n",
    "Nous allons donc remplacer ces valeurs par des \"1\" pour True et des \"0\" pour False."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.replace({True:1, False:0}, inplace=True)\n",
    "quantitative_df.replace({True:1, False:0}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Elimination des colonnes contenant trop de valeurs NaN**\n",
    "\n",
    "On compte notamment : 2010 Census Tracts & City Council Districts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['2010 Census Tracts', 'City Council Districts'], axis=1, inplace=True)\n",
    "quantitative_df.drop(['2010 Census Tracts', 'City Council Districts'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SPD Beats**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(merge_df['SPD Beats'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop('SPD Beats', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Seattle Police Department Micro Community Policing Plan Areas**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(merge_df['Seattle Police Department Micro Community Policing Plan Areas'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop('Seattle Police Department Micro Community Policing Plan Areas', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne DataYear**\n",
    "\n",
    "Cette colonne contient toujours la même valeur. Nous pouvons la rejeter sans perdre d'information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Attention, ça ne va pas du tout\n",
    "\n",
    "merge_df['DataYear_2015'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['DataYear_2016'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['DataYear_2015', 'DataYear_2016'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne NumberofBuildings_2016**\n",
    "\n",
    "Certaines données indiquent un nombre nul de bâtiment (=0) : ces données sont erronées, remplaçons-les par la valeur *None*."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['NumberofBuildings_2016'].replace(0, None, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne OtherFuelUse(kBtu)**\n",
    "\n",
    "Cette colonne ne semble pas intéressante pour notre étude :\n",
    "*   Sa description n'est pas disponible sur le site de Seattle City\n",
    "*   Elle contient une grande majorité de valeurs nulles.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['OtherFuelUse(kBtu)'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop('OtherFuelUse(kBtu)', axis=1, inplace=True)\n",
    "quantitative_df.drop('OtherFuelUse(kBtu)', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne SteamUse(kBtu)_2015**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['SteamUse(kBtu)_2015'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop('SteamUse(kBtu)_2015', axis=1, inplace=True)\n",
    "quantitative_df.drop('SteamUse(kBtu)_2015', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne SteamUse(kBtu)_2016**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['SteamUse(kBtu)_2016'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop('SteamUse(kBtu)_2016', axis=1, inplace=True)\n",
    "quantitative_df.drop('SteamUse(kBtu)_2016', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne ThirdLargestPropertyUseTypeGFA**\n",
    "\n",
    "Cette colonne contient très peu de valeurs. Il vaut mieux la rejeter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['ThirdLargestPropertyUseTypeGFA_2015'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['ThirdLargestPropertyUseTypeGFA_2015', 'ThirdLargestPropertyUseTypeGFA_2016'], axis=1, inplace=True)\n",
    "quantitative_df.drop(['ThirdLargestPropertyUseTypeGFA_2015', 'ThirdLargestPropertyUseTypeGFA_2016'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Colonne SecondLargestPropertyUseTypeGFA**\n",
    "\n",
    "Cette colonne contient assez peu de valeurs, il vaut mieux la rejeter également."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['SecondLargestPropertyUseTypeGFA_2015'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.drop(['SecondLargestPropertyUseTypeGFA_2015', 'SecondLargestPropertyUseTypeGFA_2016'], axis=1, inplace=True)\n",
    "quantitative_df.drop(['SecondLargestPropertyUseTypeGFA_2015', 'SecondLargestPropertyUseTypeGFA_2016'], axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. Exploration : what do our data contain?\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_explorator.plot_feature(merge_df, 'ENERGYSTARScore_2015')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Focus sur l'évolution de l'ENERGYSTARScore entre 2015 et 2016**\n",
    "\n",
    "Sur l'ensemble des bâtiments, l'année 2016 a-t-elle été meilleure que l'année 2015 ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_features_differences(df, column_1, column_2):\n",
    "    years_diff_series = df[column_1] - df[column_2]\n",
    "    ax = plt.axes()\n",
    "    ax.yaxis.grid()\n",
    "    plt.title('Difference between {} and {}'.format(column_1, column_2))\n",
    "    plt.hist(years_diff_series, bins=100, color='blue', edgecolor='k')\n",
    "\n",
    "plot_features_differences(merge_df, 'ENERGYSTARScore_2016', 'ENERGYSTARScore_2015')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Focus sur les étiquettes**\n",
    "\n",
    "Nous souhaitons avoir seulement deux colonnes d'étiquettes. Actuellement, nous avons 4 colonnes, 2 en 2015 et 2 en 2016.\n",
    "\n",
    "Voyons les différences entre les deux années."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_features_differences(merge_df, 'SiteEnergyUseWN(kBtu)_2016', 'SiteEnergyUseWN(kBtu)_2015')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['GHGEmissionsTotal_2015'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['GHGEmissionsTotal_2016'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_features_differences(merge_df, 'GHGEmissionsTotal_2016', 'GHGEmissionsTotal_2015')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les colonnes étiquettes présentent des caractéristiques statistiques similaires. Nous allons fusionner les deux années en calculant la moyenne par bâtiment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df['SiteEnergyUseWN_average'] = (merge_df['SiteEnergyUseWN(kBtu)_2015'] + merge_df['SiteEnergyUseWN(kBtu)_2016'])/2\n",
    "merge_df.drop('SiteEnergyUseWN(kBtu)_2015', axis=1, inplace=True)\n",
    "merge_df.drop('SiteEnergyUseWN(kBtu)_2016', axis=1, inplace=True)\n",
    "\n",
    "merge_df['GHGEmissionsTotal_average'] = (merge_df['GHGEmissionsTotal_2015'] + merge_df['GHGEmissionsTotal_2016'])/2\n",
    "merge_df.drop('GHGEmissionsTotal_2015', axis=1, inplace=True)\n",
    "merge_df.drop('GHGEmissionsTotal_2016', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_transformation(df, column):\n",
    "    new_df = df.copy()\n",
    "    new_df['log('+column+')'] = math.log(new_df[column])\n",
    "    new_df.drop(column, axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "merge_df = log_transformation(merge_df, 'GHGEmissionsTotal_average')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_explorator.plot_feature(merge_df, 'log(GHGEmissionsTotal_average)', quantile_sup=0.99)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df = log_transformation(merge_df, 'SiteEnergyUseWN_average')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c. Analysis : which of our data are useful?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Redéfinition des colonnes quantitatives"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Données quantitatives\n",
    "quantitative_columns = [column for column in merge_df.columns if merge_df[column].dtype != object]\n",
    "quantitative_df = merge_df[quantitative_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First objective is to evaluate the interest of ENERGY STAR score for emissions prediction. It leads us to choose the useful columns accordingly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GHGEmissionsIntensity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_scatter('ENERGYSTARScore_2016', 'SiteEnergyUseWN_average')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_scatter('ENERGYSTARScore_2016', 'GHGEmissionsTotal_average')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlation_consumption = {'ZipCode':[0.003656, 0.002976],\n",
    "                           'YearBuilt_2016':[0.005729, 0.000657],\n",
    "                           'YearBuilt_2015':[0.005729, 0.000657],\n",
    "                           'PropertyGFATotal':[0.433141, 0.242183],\n",
    "                           'PropertyGFABuilding(s)':[0.451066, 0.274090],\n",
    "                           'NumberofFloors_2016':[0.119185, 0.0350338],\n",
    "                           'NumberofBuildings_2015':[0.031333, 0.018412],\n",
    "                           'Longitude':[0.000992, 0.001195],\n",
    "                           'Latitude':[0.000950, 0.000284],\n",
    "                           'DefaultData':[0.000772, 0.000772],\n",
    "                           'DataYear':[0, 0],\n",
    "                           'Seattle Police Department Micro Community Policing Plan Areas':[4.70361e-05, 0.000634],\n",
    "                           'SPD Beats':[0.001655, 0.001588],\n",
    "                           'SiteEnergyUse(kBtu)':[0.996577, 0.794572],\n",
    "                           'SiteEnergyUseWN(kBtu)':[1.0, 0.797673],\n",
    "                           'SiteEUIWN(kBtu/sf)':[0.192727, 0.117020],\n",
    "                           'SiteEUI(kBtu/sf)':[0.198178, 0.119329],\n",
    "                           'SteamUse(kBtu)_2016':[0.354672, 0.117020],\n",
    "                           'SourceEUIWN(kBtu/sf)':[0.197180, 0.078598],\n",
    "                           'SourceEUI(kBtu/sf)':[0.200160, 0.081182],\n",
    "                           'NumberofBuildings_2015':[0.031334, 0.018412],\n",
    "                           'NaturalGas(therms)_2016':[0.377854, 0.427167],\n",
    "                           'GHGEmissionsTotal':[0.797673, 1.0],\n",
    "                           'GHGEmissionsIntensity':[0.122309, 0.182920],\n",
    "                           'Electricity(kWh)':[0.870812, 0.476882],\n",
    "                           'NumberofFloors_2015':[0.119906, 0.035292],\n",
    "                           'SteamUse(kBtu)_2015':[0.344648, 0.585326],\n",
    "                           'OtherFuelUse(kBtu)':[0.001977, 0.001611],\n",
    "                           'NaturalGas(therms)_2015':[0.384946, 0.427094],\n",
    "                           'LargestPropertyUseTypeGFA':[0.495779, 0.333422],\n",
    "                           'ENERGYSTARScore':[0.006413, 0.008733]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correl_consum_df =  pd.DataFrame(data=correlation_consumption).transpose()\n",
    "correl_consum_df.sort_values(by=[1], inplace=True)\n",
    "temp_length = len(correl_consum_df)\n",
    "\n",
    "plt.figure(figsize=(math.sqrt(temp_length*2), math.sqrt(temp_length*2)))\n",
    "plt.title('Regression coefficient toward GHGEmissionsTotal')\n",
    "plt.barh(correl_consum_df.index, correl_consum_df[1], color='blue', edgecolor='k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comparaison des corrélations entre colonnes quantitatives**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_explorator.quant_heatmap(quantitative_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Supprimons les colonnes blanches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blank_columns = ['Comments_2016', 'ENERGYSTAR_2001']\n",
    "quantitative_df.drop(blank_columns, axis=1, inplace=True)\n",
    "merge_df.drop(blank_columns, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_explorator.quant_heatmap(quantitative_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Quelques observations**\n",
    "\n",
    "Certaines colonnes sont trop correlées entre elles (coefficient de corrélation > 0,9).\n",
    "\n",
    "N.B.: C'est le signe que notre choix de dédoubler certaines colonnes, lors de la fusion des tableaux 2015 et 2016, était inadapté.\n",
    "\n",
    "De plus, l'enjeu des modélisations à venir est de se passer des relevés annuels, c'est à des colonnes suivantes :\n",
    "Electricity, GHGEmissionsIntensity, NaturalGas, SiteEUI, SiteEUIWN, SiteEnergyUse, SourceEUI, SourceEUIWN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correl_columns = ['CouncilDistrictCode_2015',\n",
    "                  'ENERGYSTARScore_2015',\n",
    "                  'ENERGYSTAR_2002',\n",
    "                  'Electricity(kBtu)_2015',\n",
    "                  'Electricity(kBtu)_2016',\n",
    "                  'Electricity(kWh)_2015',\n",
    "                  'Electricity(kWh)_2016',\n",
    "                  'GHGEmissionsIntensity_2015',\n",
    "                  'GHGEmissionsIntensity_2016',\n",
    "                  'LargestPropertyUseTypeGFA_2015',\n",
    "                  'NaturalGas(therms)_2015',\n",
    "                  'NaturalGas(therms)_2016',\n",
    "                  'NaturalGas(kBtu)_2015',\n",
    "                  'NaturalGas(kBtu)_2016',\n",
    "                  'NumberofFloors_2015',\n",
    "                  'PropertyGFABuilding(s)_2015',\n",
    "                  'PropertyGFABuilding(s)_2016',\n",
    "                  'PropertyGFAParking_2015',\n",
    "                  'PropertyGFATotal_2015',\n",
    "                  'PropertyGFATotal_2016',\n",
    "                  'SiteEUI(kBtu/sf)_2015',\n",
    "                  'SiteEUI(kBtu/sf)_2016',\n",
    "                  'SiteEUIWN(kBtu/sf)_2015',\n",
    "                  'SiteEUIWN(kBtu/sf)_2016',\n",
    "                  'SiteEnergyUse(kBtu)_2015',\n",
    "                  'SiteEnergyUse(kBtu)_2016',\n",
    "                  'SourceEUI(kBtu/sf)_2015',\n",
    "                  'SourceEUI(kBtu/sf)_2016',\n",
    "                  'SourceEUIWN(kBtu/sf)_2015',\n",
    "                  'SourceEUIWN(kBtu/sf)_2016',\n",
    "                  'YearBuilt_2015',\n",
    "                  'ZipCode_2015',# Identifiant arbitraire\n",
    "                  'ZipCode_2016'# Identifiant arbitraire\n",
    "                  ]\n",
    "quantitative_df.drop(correl_columns, axis=1, inplace=True)\n",
    "merge_df.drop(correl_columns, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_explorator.quant_heatmap(quantitative_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "merge_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(merge_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# V.  Modélisations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0 Introduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons scinder nos modélisations en deux parties :\n",
    "- les modélisations visant à prédire la **consommation en énergie**,\n",
    "- les modélisations visant à prédire les **émissions de CO2**,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Emissions de CO2 et consommation d'énergie sont des données **qualitatives**. Il s'agit donc d'une problématique de **régression** (et non de classification)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous évaluerons les modèles choisis à l'aide de trois critères :\n",
    "- la **précision**, estimée via des méthodes propres à scikitlearn,\n",
    "- le **temps de calcul**, estimé via un simple %timeit,\n",
    "- la **complexité algorithmique**."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Déclaration de la matrice X. On ne prend pas en compte les targets.\n",
    "merge_df = merge_df.dropna()\n",
    "X = merge_df.drop(['log(SiteEnergyUseWN_average)', 'log(GHGEmissionsTotal_average)'], axis=1)\n",
    "\n",
    "# Déclaration des vecteurs targets\n",
    "y_consumptions = merge_df['log(SiteEnergyUseWN_average)']\n",
    "y_emissions = merge_df['log(GHGEmissionsTotal_average)']\n",
    "targets = y_consumptions\n",
    "temp_title = targets.columns[0][4:-9]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fonction d'activation de la caractéristique ENERGYSTARScore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# On active ou non la ligne suivant que l'on veut rejeter ou garder la caractéristique ENERGYSTARScore\n",
    "if False:\n",
    "    X.drop(['ENERGYSTARScore_2016',\n",
    "            'ENERGYSTAR_2000','ENERGYSTAR_2003','ENERGYSTAR_2004','ENERGYSTAR_2005','ENERGYSTAR_2006',\n",
    "            'ENERGYSTAR_2007','ENERGYSTAR_2008','ENERGYSTAR_2009','ENERGYSTAR_2010','ENERGYSTAR_2011',\n",
    "            'ENERGYSTAR_2012','ENERGYSTAR_2013','ENERGYSTAR_2014','ENERGYSTAR_2015','ENERGYSTAR_2016'],\n",
    "           axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La sélection de modèle montre que les performances de prédiction sont meilleures avec les ENERGYSTARScore. Nous allons les garder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Feature Engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. One-hot encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le OneHotEncoder s'applique aux données catégorielles (qualitatives)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_pareto(X, 'BuildingType_2015')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_qualitative.drop('OSEBuildingID', axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X_qualitative)\n",
    "X_encoded = X_encoded.toarray()\n",
    "X_encoded.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le OneHotEncoder transforme les colonnes catégorielles en 102 colonnes.\n",
    "\n",
    "Cela semble beaucoup, voyons si nous pouvons obtenir un meilleur résultat en modifiant la modélisation : nous allons rejeter les éléments comptant pour moins de 1% de la colonne (c'est-à-dire, pour une colonne de 1491 éléments, moins de 14 occurences). Ils sont à l'origine de colonnes contenant un \"1\", puis le reste de \"0\", et ont une faible utilité pour les modélisations futures."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Repérage des valeurs de chaque colonne présentes à plus de n%\n",
    "df_length = len(X_qualitative)\n",
    "rate = 0.01\n",
    "representative_values =  {}\n",
    "for column in X_qualitative:\n",
    "    temp_series = X_qualitative[column]\n",
    "    values_count = temp_series.value_counts()\n",
    "    representative_values[column] = [element for element in temp_series.unique() if values_count[element] > rate * df_length]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_v2 = OneHotEncoder(handle_unknown='ignore', categories=list(representative_values.values()))\n",
    "X_encoded = encoder_v2.fit_transform(X_qualitative)\n",
    "X_encoded = X_encoded.toarray()\n",
    "X_encoded.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X['OSEBuildingID'] = X['OSEBuildingID'].astype('int')\n",
    "qualitative_columns = X.select_dtypes(['object']).columns\n",
    "onehot_qualitative_columns = encoder_v2.get_feature_names(input_features=None)\n",
    "X_qualitative = pd.DataFrame(X_transformed, columns=onehot_qualitative_columns)\n",
    "X_qualitative.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(X_qualitative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.drop(qualitative_columns, axis=1, inplace=True)\n",
    "X = pd.merge(X.reset_index(), X_qualitative,\n",
    "             left_index=True, right_index=True)\n",
    "# On remplace les espaces vides par des underscores pour éviter les éventuels dysfonctionnements lors de l'utilisation de XGBoost\n",
    "X.rename(columns = lambda x: x.replace(' ', '_'), inplace=True)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. Standard Scaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# On écarte les colonnes créées par le OneHotEncoder, qui commencent par x0_ ..., x1_ ...\n",
    "X_quantitative = X.filter(regex=('^(?!x\\d+_)'))\n",
    "quantitative_columns = X_quantitative.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Distributions des features quantitatives**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.distplot(X_quantitative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Standardisation des colonnes quantitatives**\n",
    "\n",
    "Pour chaque colonne de caractéristique quantitative, on ramène la plus grande partie des valeurs dans l'intervalle [-1 ; 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scale_dataframe(df):\n",
    "    # Scale quantitative columns\n",
    "    scaler = StandardScaler()\n",
    "    df_quant = df.filter(regex=('^(?!x\\d+_)'))\n",
    "    df_quant = scaler.fit_transform(df_quant)\n",
    "    df_quant = pd.DataFrame(df_quant, columns=df_quant.columns)\n",
    "    # Modify the original dataframe\n",
    "    df.drop(df_quant.columns, axis=1, inplace=True)\n",
    "    for column in df_quant.columns:\n",
    "        df[column] = df_quant[column]\n",
    "    return df\n",
    "\n",
    "\n",
    "X_quantitative = scale_quantitative_dataframe(X_quantitative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Distribution des features quantitaives après StandardScaler**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.distplot(X_quantitative)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NaN_proportion(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Model Selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Préparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chaque modèle sera optimisé et évalué selon les étapes suivantes :\n",
    "\n",
    "1.   **Split** 80/20 de l'ensemble de départ (train set / test set)\n",
    "2.   Réduction en composants principaux (**PCA**), puis split 80/20 de nouveau\n",
    "1.   Validation croisée (**CV**)\n",
    "2.   Validation croisée mélangée (**SCV**)\n",
    "1.   Utilisation d'un sous-ensemble de validation pour trouver les paramètres optimaux (**Validation Set**)\n",
    "2.   Recherche sur grille (**GridSearchCV**)\n",
    "2.   Validation croisée imbriquée (**GridSearchCV + CV** & **Gridsearch + SCV**)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Performance dataframe on test set\n",
    "scores_df = pd.DataFrame(columns=['Model', 'Config', 'Métrique', 'Score', 'Rapidité'])\n",
    "# Performance dataframe on train set\n",
    "scores_train_df = pd.DataFrame(columns=['Model', 'Config', 'Métrique', 'Score', 'Rapidité'])\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, test_size=0.2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def best_PCA_number(metrics, model, range_max, range_min, data=X, range_step=1):\n",
    "    '''Performances de prédiction d'un modèle selon le nombre de composants principaux'''\n",
    "    pca_mse = {}\n",
    "    pca_score = {}\n",
    "    # Récupérer les valeurs\n",
    "    for n_components in range(range_min, range_max, range_step):\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(data)\n",
    "        Xtrain_pca, Xtest_pca, ytrain_pca, ytest_pca = train_test_split(X_pca, targets,\n",
    "                                                                        test_size=0.2, random_state=1)\n",
    "        reg = model.fit(Xtrain_pca, ytrain_pca)\n",
    "        if metrics_ == 'mse':\n",
    "            model_mse = mean_squared_error(y_true=ytest_pca, y_pred=reg.predict(Xtest_pca))\n",
    "            pca_mse[n_components] = model_mse\n",
    "        elif metrics_ == 'score':\n",
    "            model_score = reg.score(Xtest_pca, ytest_pca)\n",
    "            pca_score[n_components] = model_score\n",
    "    # Graph\n",
    "    plt.xlabel('Nombre de composants principaux')\n",
    "    plt.xticks(range(range_min, range_max))\n",
    "    ax = plt.axes()\n",
    "    if metrics_ == 'mse':\n",
    "        plt.ylim(0, 10)\n",
    "        plt.title('Erreur quadratique en fonction du nombre de composants principaux')\n",
    "        plt.ylabel('Erreur quadratique')\n",
    "    elif metrics_ == 'score':\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.xaxis.tick_top()\n",
    "        plt.ylabel('Score du modèle')\n",
    "        plt.ylim(-10, 2)\n",
    "        plt.title('Score moyen en fonction du nombre de composants principaux')\n",
    "    ax.yaxis.grid()\n",
    "    plt.bar(list(pca_score.keys()), list(pca_score.values()),\n",
    "                 color='orange', edgecolor='k')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a LinearRegression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'idée est de déterminer une relation du type :\n",
    "\n",
    "y[k] = w[0] \\* x[k, 0] + w[1] \\* x[k, 1] + ....... + w[p] \\* x[k, p]\n",
    "\n",
    "où y[k] est la k-ième target du vecteur target y, et (x[k, 0], x[k, 1], ..., x[k, p]) sont les éléments de la matrice X, correspondant à la k-ième ligne de la matrice."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enregistrement du nom du modèle\n",
    "temp_model = 'LinearRegression'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Enregistrement du nom de la configuration (étape d'optimisation)\n",
    "temp_config = 'Split'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Métrique R2\n",
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "lr = LinearRegression()\n",
    "reg = lr.fit(X_train, y_train)\n",
    "temp_value = round(reg.score(X_test, y_test), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Métrique MSE\n",
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "lr = LinearRegression()\n",
    "reg = lr.fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_true=y_test, y_pred=reg.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. PCA**\n",
    "\n",
    "Comment déterminer le nombre de composants principaux ? Lequel donnera la meilleure performance ? Nous allons calculer la performance du modèle pour différents nombres de composants principaux."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'PCA'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "best_PCA_number(model_=LinearRegression(),\n",
    "                metrics_='mse',\n",
    "                data_=X,\n",
    "                range_min_=2,\n",
    "                range_max_=30)\n",
    "pca_delta = time.time() - time0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le meilleur score est obtenu pour 7 composants principaux, et ce pour les deux métriques R2 et RMSE : calculons les performances avec ce nombre."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1\n",
    "pca = PCA(n_components=7)\n",
    "X_PCA = pca.fit_transform(X)\n",
    "lr = LinearRegression()\n",
    "Xtrain_pca, Xtest_pca, ytrain_pca, ytest_pca = train_test_split(X_PCA, targets,\n",
    "                                                                test_size=0.2, random_state=1)\n",
    "reg = lr.fit(Xtrain_pca, ytrain_pca)\n",
    "temp_value = round(reg.score(Xtest_pca, ytest_pca), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "pca = PCA(n_components=7)\n",
    "X_PCA = pca.fit_transform(X)\n",
    "lr = LinearRegression()\n",
    "Xtrain_pca, Xtest_pca, ytrain_pca, ytest_pca = train_test_split(X_PCA,\n",
    "                                                                targets,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=1)\n",
    "reg = lr.fit(Xtrain_pca, ytrain_pca)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_true=ytest_pca, y_pred=reg.predict(Xtest_pca))), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les performances sont meilleures avec le PCA. Nous allons l'utiliser."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nb_fold = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr,\n",
    "                         Xtrain_pca,\n",
    "                         ytrain_pca,\n",
    "                         cv=nb_fold,\n",
    "                         scoring='r2')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr,\n",
    "                         Xtrain_pca,\n",
    "                         ytrain_pca,\n",
    "                         cv=nb_fold,\n",
    "                         scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=nb_fold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr,\n",
    "                         Xtrain_pca,\n",
    "                         ytrain_pca,\n",
    "                         cv=kfold,\n",
    "                         scoring='r2')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr,\n",
    "                         Xtrain_pca,\n",
    "                         ytrain_pca,\n",
    "                         cv=kfold,\n",
    "                         scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons ajouter une contrainte au modèle de régression linéaire : nous allons faire en sorte de minimiser l'ensemble des coefficients de pondération w[i].\n",
    "\n",
    "En **complexifiant** ainsi notre modèle, nous le rendons certes plus difficile à gérer, mais plus **capable de coller** aux données et à leur variabilité."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'Ridge'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "temp_value = round(ridge.score(X_test, y_test), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. PCA**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le nombre de 10 composants principaux (PCA) a été déterminé au chapitre précédent pour le modèle LinearRegression. Déterminons-le pour le modèle Ridge."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'PCA'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "best_PCA_number(model=Ridge(), metrics='score',\n",
    "                data=X, range_min=2, range_max=30)\n",
    "pca_delta = time.time() - time0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "pca = PCA(n_components=7)\n",
    "ridge = Ridge().fit(Xtrain_pca, ytrain_pca)\n",
    "temp_value = round(ridge.score(Xtest_pca, ytest_pca), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "pca = PCA(n_components=7)\n",
    "ridge = Ridge().fit(Xtrain_pca, ytrain_pca)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_true=ytest_pca, y_pred=ridge.predict(Xtest_pca))), 2)\n",
    "time_delta = round(time.time() - time1 + pca_delta, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le PCA ne semble pas profiter au modèle Ridge, et détériore ses performances. Nous n'allons pas l'utiliser pour les validations croisées."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "ridge = Ridge()\n",
    "scores = cross_val_score(ridge, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "ridge = Ridge()\n",
    "scores = cross_val_score(ridge, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "ridge = Ridge()\n",
    "scores = cross_val_score(ridge, X_train, y_train,\n",
    "                         cv=kfold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "ridge = Ridge()\n",
    "scores = cross_val_score(ridge, X_train, y_train,\n",
    "                         cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c Lasso"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans ce modèle, nous reprenons la regression linéaire et, un peu de la même manière qu'avec le Ridge, nous allons ajouter une contrainte aux coefficients.\n",
    "\n",
    "Mais pour le Lasso, le calcul de la regression linéraire est fait de telle manière que les valeurs absolues des coefficients sont réduites individuellement. A la différence du Ridge, certains coefficients peuvent être réduits à zéro. Cela conduit à une sélection automatique des caractéristiques."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'Lasso'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "temp_value = round(lasso.score(X_test, y_test), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_true=y_test, y_pred=lasso.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. PCA**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'PCA'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_PCA_number(model_=Lasso(), metrics_='mse', data_=X,\n",
    "                range_min_=2, range_max_=15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le PCA ne semble pas apporter de la précision à notre modèle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "temp_value = 'Not relevant'\n",
    "time_delta = 'Not relevant'\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "temp_value = 'Not relevant'\n",
    "time_delta = 'Not relevant'\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(lasso, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(lasso, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'\n",
    "kfold = KFold(n_splits=nb_fold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(lasso, X_train, y_train,\n",
    "                         cv=kfold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(lasso, X_train, y_train,\n",
    "                         cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d ElasticNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'ElasticNet est une sorte de combinaison des deux modèles précédents, Ridge et Lasso.\n",
    "\n",
    "Il prend des hyperparamètres régulant ces deux modèles, ce qui amène à estimer ses performances à l'aide d'une GridSearch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'ElasticNet'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "elasnet = ElasticNet().fit(X_train, y_train)\n",
    "temp_value = round(elasnet.score(X_test, y_test), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "elasnet = ElasticNet().fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_test, elasnet.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. PCA**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'PCA'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_PCA_number(model_=ElasticNet(), metrics_='score', data_=X,\n",
    "                range_min_=2, range_max_=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le PCA ne semble pas apporter de précision supplémentaire à notre modèle."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "temp_value = 'Not relevant'\n",
    "time_delta = 'Not relevant'\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "temp_value = 'Not relevant'\n",
    "time_delta = 'Not relevant'\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "elasnet = ElasticNet()\n",
    "scores = cross_val_score(elasnet, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "elasnet = ElasticNet()\n",
    "scores = cross_val_score(elasnet, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'\n",
    "kfold = KFold(n_splits=nb_fold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "Elasnet = ElasticNet()\n",
    "scores = cross_val_score(elasnet, X_train, y_train,\n",
    "                         cv=kfold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "Elasnet = ElasticNet()\n",
    "scores = cross_val_score(elasnet, X_train, y_train,\n",
    "                         cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e DecisionTreeRegressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'exemple du DecisionTreeRegressor sur le site internet de scikit-learn est donné pour deux composants."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'DecisionTreeRegressor'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "DecTree = DecisionTreeRegressor(min_samples_split=3)\n",
    "DecTree.fit(X_train, y_train)\n",
    "temp_value = round(DecTree.score(X_test, y_test), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "DecTree = DecisionTreeRegressor(min_samples_split=3)\n",
    "DecTree.fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_test, DecTree.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(DecTree, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(DecTree, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(DecTree, X_train, y_train,\n",
    "                         cv=kfold, scoring='r2')\n",
    "temp_score = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "scores = cross_val_score(DecTree, X_train, y_train,\n",
    "                         cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "temp_rmse = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### f RandomForest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons construire de nombreux arbres de décision, chacun devant faire un travail de prédiction acceptable, tout en étant différent des autres arbres. L'expression forêt aléatoire vient de cette multiplication des arbres, ainsi que de l'injection d'une part d'aléatoire dans leur construction afin de s'assurer qu'ils sont tous différents.\n",
    "\n",
    "Source : \"Le Machine Learning avec Python\", Mueller & Guido"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'RandomForest'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les RandomForestRegressor n'acceptent que mse et mae comme métriques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "RFR = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=50)\n",
    "RFR.fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_test, RFR.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. PCA**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le modèle nécessite trop de temps de calcul pour pouvoir faire trop de simulations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "RFR = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=50)\n",
    "scores = cross_val_score(RFR, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "RFR = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=50)\n",
    "scores = cross_val_score(RFR, X_train, y_train,\n",
    "                         cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### g GradientBoosting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A la différence de l'approche utilisée pour les forêts aléatoires, le *gradient boosting* fonctionne en construisant des arbres de manière sérielle, chaque arbre essayant de corriger les erreurs faites par le précédent.\n",
    "\n",
    "Source : \"Le Machine Learning avec Python\", Mueller & Guido"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'GradientBoosting'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split 80/20**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "temp_value = round(gbrt.score(X_test, y_test), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_test, gbrt.predict(X_test))), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La performance est meilleure sur le jeu d'entraînement que sur le jeu de test : il y a peut-être un surapprentissage.\n",
    "\n",
    "Voyons si nous pouvons réduire ce surapprentissage, en agissant notamment sur la profondeur maximale des arbres et sur le niveau d'apprentissage d'un arbre à l'autre (learning_rate), dans la partie Gridsearch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Validation croisée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'CV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "scores = cross_val_score(gbrt, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "scores = cross_val_score(gbrt, X_train, y_train,\n",
    "                         cv=nb_fold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "scores = cross_val_score(gbrt, X_train, y_train,\n",
    "                         cv=kfold, scoring='r2')\n",
    "temp_value = round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "gbrt = GradientBoostingRegressor(random_state=0)\n",
    "scores = cross_val_score(gbrt, X_train, y_train,\n",
    "                         cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "temp_value = -round(scores.mean(), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### h XGBoost sur forêt aléatoire"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"XGBoost is normally used to train gradient-boosted decision trees and other gradient boosted models.  (...) One can use XGBoost to train a standalone random forest.\"\n",
    "\n",
    "Source : https://xgboost.readthedocs.io/en/latest/tutorials/rf.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'XGBoost'\n",
    "data_dmatrix = xgb.DMatrix(data=X, label=targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Split**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Source : https://www.datacamp.com/community/tutorials/xgboost-in-python"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Split'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "xg_reg = xgb.XGBRegressor(# General parameters\n",
    "                          colsample_bytree = 0.3,\n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 5,\n",
    "                          n_estimators = 10,\n",
    "                          objective ='reg:squarederror',\n",
    "                          # Regularization parameters\n",
    "                          alpha = 10)\n",
    "                          # Learning tasks parameters : None\n",
    "xg_reg.fit(X_train,y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "temp_value = round(np.sqrt(mean_squared_error(y_test, preds)), 2)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb.plot_tree(xg_reg,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Validation croisée mélangée**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'SCV'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_metric = 'RMSE'\n",
    "time1 = time.time()\n",
    "params = {'objective':'reg:squarederror',\n",
    "          'colsample_bytree':0.3,\n",
    "          'learning_rate':0.1,\n",
    "          'max_depth':5,\n",
    "          'n_estimators':10,\n",
    "          'alpha':10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix,\n",
    "                   params=params,\n",
    "                   nfold=5,\n",
    "                   num_boost_round=50,\n",
    "                   early_stopping_rounds=10,\n",
    "                   metrics='rmse',\n",
    "                   as_pandas=True,\n",
    "                   seed=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_value = cv_results['test-rmse-mean'][len(cv_results)-1]\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_train_df.loc[len(scores_train_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### i Dummy Regressor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_model = 'DummyRegressor'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Mean'\n",
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "temp_value = dummy_regr.score(X_test, y_test)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp_config = 'Median'\n",
    "temp_metric = 'R2'\n",
    "time1 = time.time()\n",
    "dummy_regr = DummyRegressor(strategy=\"median\")\n",
    "dummy_regr.fit(X_train, y_train)\n",
    "temp_value = dummy_regr.score(X_test, y_test)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "scores_df.loc[len(scores_df)] = [temp_model, temp_config, temp_metric, temp_value, time_delta]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Bilan de performance des modèles utilisés"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_list = scores_df['Model'].unique()\n",
    "config_list = scores_df['Config'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df.replace(['Not relevant', 'Too long'], np.nan, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df[scores_df['Métrique']=='R2']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_train_df[scores_train_df['Métrique']=='RMSE']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Comparaisons des modèles**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_r2_value = max(scores_df[scores_df['Métrique']=='R2']['Score'])\n",
    "best_rmse_value = min(abs(scores_df[scores_df['Métrique']=='RMSE']['Score']))\n",
    "best_r2_model = scores_df[scores_df['Score']==best_r2_value]['Model'].unique()[0]\n",
    "best_rmse_model = scores_df[scores_df['Score']==best_rmse_value]['Model'].unique()[0]\n",
    "print('Best r2 score: {}, model {}.'.format(best_r2_value, best_r2_model))\n",
    "print('Best rmse score: {}, model {}.'.format(best_rmse_value, best_rmse_model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Problème : le modèle le plus précis n'est pas forcément le plus rapide. Avec la complexité algorithmique, précision et rapidité sont les indicateurs les plus importants, nous allons les comparer en même temps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualisation des prédictions avec la métrique RMSE\n",
    "rmse_df = scores_df[scores_df['Métrique']=='RMSE']\n",
    "rmse_scores = rmse_df.sort_values(by=['Score'], ascending = False)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.title(temp_title)\n",
    "plt.xlabel('Score RMSE')\n",
    "plt.ylabel('Rapidité')\n",
    "temp_x = list(rmse_scores['Score'])\n",
    "temp_y = list(rmse_scores['Rapidité'])\n",
    "plt.xlim(-0.08, 1.08)\n",
    "plt.ylim(-0.5, 2.5)\n",
    "plt.grid(True)\n",
    "plt.scatter(temp_x, temp_y, color='orange', edgecolor='k', s=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le bilan des performances montre que le GradientBoosting donne le meilleur compromis précision / rapidité.\n",
    "\n",
    "Cela dit le XGBoost est réputé donner les meilleures performances la majorité des cas. Nous allons donc optimiser GradientBoosting et XGBoost."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_n_features = X_train.shape[1]\n",
    "n_features = X.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a.i Gradient Boosting : GridsearchCV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':[100, 1000],\n",
    "              'learning_rate':[0.01, 0.2],\n",
    "              'max_depth':[2, 5]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "GBing = GradientBoostingRegressor(max_features=int(np.sqrt(train_n_features)))\n",
    "rdm = GridSearchCV(GBing, param_grid=param_grid,\n",
    "                   cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "rdm.fit(X_train, y_train)\n",
    "rdm_best = rdm.best_estimator_\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "train_score = round(mean_squared_error(y_train, rdm_best.predict(X_train)), 2)\n",
    "test_score = round(mean_squared_error(y_test, rdm_best.predict(X_test)), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Best parameters:', rdm.best_params_,\n",
    "      '\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a.ii GradientBoosting : RandomizedSearchCV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Etape 1**\n",
    "\n",
    "Fixer certains paramètres pour ne plus les avoir dans la RandomizedSearchCV, et ainsi réduire le temps de calcul."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distributions = dict(n_estimators=[250, 500, 750],\n",
    "                     learning_rate=uniform(loc=0.02, scale=1.98),\n",
    "                     max_depth=[2, 3, 4],\n",
    "                     alpha=uniform(loc=0.1, scale=0.9))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "GBing = GradientBoostingRegressor(max_features=int(np.sqrt(train_n_features)),\n",
    "                                  random_state=0)\n",
    "rdm = RandomizedSearchCV(GBing, distributions, cv=kfold,\n",
    "                         n_iter=2, # Normalement 60, passé à 2 pour ne pas ralentir le programme\n",
    "                         scoring='neg_root_mean_squared_error', random_state=0)\n",
    "rdm.fit(X_train, y_train)\n",
    "rdm_best = rdm.best_estimator_\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "train_score = round(mean_squared_error(y_train, rdm_best.predict(X_train)), 2)\n",
    "test_score = round(mean_squared_error(y_test, rdm_best.predict(X_test)), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Best parameters:', rdm.best_params_,\n",
    "      '\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Etape 2** : faisons entrer d'autres paramètres en jeu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distributions = dict(n_estimators=randint(100, 1000),\n",
    "                     learning_rate=uniform(loc=0.02, scale=1.98),\n",
    "                     max_depth=randint(2, 20),\n",
    "                     alpha=uniform(loc=0.1, scale=0.9),\n",
    "                     min_samples_leaf=randint(2, 20),\n",
    "                     min_samples_split=randint(2, 20))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "GBing = GradientBoostingRegressor(max_features=int(np.sqrt(train_n_features)),\n",
    "                                  #n_estimators=500,\n",
    "                                  random_state=0)\n",
    "rdm = RandomizedSearchCV(GBing, distributions, cv=kfold,\n",
    "                         n_iter=2, # Normalement 60, passé à 2 pour ne pas ralentir le programme\n",
    "                         scoring='neg_root_mean_squared_error', random_state=0)\n",
    "rdm.fit(X_train, y_train)\n",
    "rdm_best = rdm.best_estimator_\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "train_score = round(mean_squared_error(y_train, rdm_best.predict(X_train)), 2)\n",
    "test_score = round(mean_squared_error(y_test, rdm_best.predict(X_test)), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Best parameters:', rdm.best_params_,\n",
    "      '\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a.iii GradientBoosting : recherche manuelle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "GBing = GradientBoostingRegressor(min_samples_split=5,\n",
    "                                  min_samples_leaf=10,\n",
    "                                  max_depth=2,\n",
    "                                  max_features=int(np.sqrt(train_n_features)),\n",
    "                                  learning_rate=0.08,\n",
    "                                  n_estimators=1250,\n",
    "                                  alpha=0.34,\n",
    "                                  random_state=0)\n",
    "GBing.fit(X_train, y_train)\n",
    "time_delta = round(time.time() - time1, 2)\n",
    "train_score = round(np.sqrt(mean_squared_error(y_train, GBing.predict(X_train))), 2)\n",
    "test_score = round(np.sqrt(mean_squared_error(y_test, GBing.predict(X_test))), 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La recherche manuelle donne parfois de meilleurs résultats que GridSearchCV et RandomizedSearchCV. Mais elle a l'air de dépendre des ensembles train/test, qui varient à chaque fois que l'on démarre le programme depuis le début.\n",
    "\n",
    "En effet, les ensembles X_train, y_train, X_test et y_test sont définis de manière aléatoire au cours du programme (dans le chapitre V.0), et ainsi font changer les performances d'un simple split."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b.i XGBoost : GridSearchCV\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params = {'eta':[0.05, 0.1, 0.2],\n",
    "          'colsample_bytree':[0.1, 0.2, 0.5],\n",
    "          'max_depth':[2, 3, 4],\n",
    "          'alpha':[1, 2, 5]}\n",
    "data_dmatrix = xgb.DMatrix(data=X, label=targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "gscv = GridSearchCV(xg_reg, param_grid=params,\n",
    "                   cv=kfold, scoring='neg_root_mean_squared_error')\n",
    "gscv.fit(X_train, y_train)\n",
    "gscv_best = gscv.best_estimator_\n",
    "train_score = round(mean_squared_error(y_train, gscv_best.predict(X_train)), 2)\n",
    "test_score = round(mean_squared_error(y_test, gscv_best.predict(X_test)), 2)\n",
    "time_delta = round(time.time() - time1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Best parameters:', gscv.best_params_,\n",
    "      '\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b.ii XGBoost : RandomizedSearchCV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distributions = dict(eta=uniform(loc=0.01, scale=1.98),\n",
    "                     #gamma=list(range(1, 8)),\n",
    "                     max_depth=list(range(2, 11)),\n",
    "                     subsample=uniform(loc=0.01, scale=0.98),\n",
    "                     #colsample_bytree=uniform(loc=0.01, scale=0.98),\n",
    "                     alpha=uniform(loc=0.01, scale=0.98),\n",
    "                     n_estimators=[100, 200, 350])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "gscv = RandomizedSearchCV(xg_reg,\n",
    "                         distributions,\n",
    "                         cv=kfold,\n",
    "                         n_iter=2, # Normalement 60, passé à 2 pour ne pas ralentir le programme\n",
    "                         scoring='neg_root_mean_squared_error')\n",
    "gscv.fit(X_train, y_train)\n",
    "gscv_best = gscv.best_estimator_\n",
    "train_score = round(mean_squared_error(y_train, gscv_best.predict(X_train)), 2)\n",
    "test_score = round(mean_squared_error(y_test, gscv_best.predict(X_test)), 2)\n",
    "time_delta = round(time.time() - time1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Best parameters:', gscv.best_params_,\n",
    "      '\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b.iii XGBoost : recherche manuelle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params= {'eta':0.52,\n",
    "         'max_depth':2,\n",
    "         #'subsample':0.5,\n",
    "         'alpha':0.75,\n",
    "         'n_estimators':200}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(params=params,\n",
    "                          objective ='reg:squarederror')\n",
    "xg_reg.fit(X_train,y_train)\n",
    "train_score = round(mean_squared_error(y_train, xg_reg.predict(X_train)), 2)\n",
    "test_score = round(mean_squared_error(y_test, xg_reg.predict(X_test)), 2)\n",
    "time_delta = round(time.time() - time1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\\nTrain score: {} / test score: {}'.format(train_score, test_score),\n",
    "      '\\nTime: {} s'.format(time_delta))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b.iv Cross validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time1 = time.time()\n",
    "#xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix,\n",
    "                       params=params,\n",
    "                       nfold=5,\n",
    "                       num_boost_round=200,\n",
    "                       early_stopping_rounds=10,\n",
    "                       metrics='rmse',\n",
    "                       as_pandas=True,\n",
    "                       seed=123)\n",
    "train_score = np.sqrt(cv_results['train-rmse-mean'][len(cv_results)-1])\n",
    "train_score = round(train_score, 2)\n",
    "test_score = np.sqrt(cv_results['test-rmse-mean'][len(cv_results)-1])\n",
    "test_score = round(test_score, 2)\n",
    "time_delta = round(time.time() - time1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VI. Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Gradient Boosting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Représentation graphique**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(color_codes=True)\n",
    "x = y_test\n",
    "y = GBing.predict(X_test)\n",
    "sns.jointplot(x, y, kind=\"reg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Représentation graphique**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(color_codes=True)\n",
    "x = y_test\n",
    "y = xg_reg.predict(X_test)\n",
    "sns.jointplot(x, y, kind=\"reg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}